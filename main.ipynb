{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📦 Step 1: Install Required Packages\n",
    "\n",
    "Before you start using this notebook, make sure all the necessary Python packages are installed. These packages are listed in the `requirements.txt` file.\n",
    "\n",
    "## 💻 How to Install\n",
    "\n",
    "Open your terminal or command prompt, navigate to the project directory (where `requirements.txt` is located), and run:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "This command will automatically install all dependencies needed for the notebook and project to run smoothly.\n",
    "\n",
    "> ✅ **Tip:** If you're using a virtual environment (recommended), make sure it's activated before running the command.\n",
    "\n",
    "## 🛠️ Troubleshooting\n",
    "\n",
    "* If you get a \"permission denied\" error, try adding `--user`:\n",
    "\n",
    "  ```bash\n",
    "  pip install --user -r requirements.txt\n",
    "  ```\n",
    "\n",
    "* If you're working in Jupyter and want to install directly from a notebook cell, you can run:\n",
    "\n",
    "  ```python\n",
    "  !pip install -r requirements.txt\n",
    "  ```"
   ],
   "id": "388df311542bae67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🔧 Second Step: Create and Register Your Project Folder\n",
    "\n",
    "In this step, you'll register the **main project folder**, so the system knows where to store and retrieve all files related to your project.\n",
    "\n",
    "This setup automatically creates a structured folder system under your specified `main_folder`, and stores the configuration in a central JSON file. This ensures your projects remain organized, especially when working with multiple studies or datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Project Folder Structure\n",
    "\n",
    "On the first run, the following folder structure will be automatically created under your specified `main_folder`:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│   └── <project_name>_database.xlsx  ← auto-generated path (not file creation)\n",
    "├── pdf/\n",
    "└── xml/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Example\n",
    "\n",
    "Suppose your project name is `corona_discharge`, and you want to store all project files under:\n",
    "\n",
    "```\n",
    "G:\\My Drive\\research_related\\ear_eog\n",
    "```\n",
    "\n",
    "You can register this setup by running:\n",
    "\n",
    "```python\n",
    "project_folder(\n",
    "    project_review='corona_discharge',\n",
    "    main_folder=r'G:\\My Drive\\research_related\\ear_eog'\n",
    ")\n",
    "```\n",
    "\n",
    "✅ This will:\n",
    "\n",
    "* Save the project path in `setting/project_folders.json`\n",
    "* Create the full folder structure: `database/scopus`, `pdf`, and `xml`\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Loading the Project Later\n",
    "\n",
    "Once registered, you can access the project folders in future sessions by providing just the `project_review` name:\n",
    "\n",
    "```python\n",
    "paths = project_folder(project_review='corona_discharge')\n",
    "\n",
    "print(paths['main_folder'])  # Main project folder\n",
    "print(paths['csv_path'])     # Path to the Excel database file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ What Happens in the Background?\n",
    "\n",
    "* A file named `project_folders.json` is stored in the `setting/` directory within your project.\n",
    "* It maps each `project_review` to its corresponding `main_folder`.\n",
    "* Folder structure is created automatically on the first run.\n",
    "* On subsequent runs, the system reads the JSON to locate your project — no need to re-enter paths.\n"
   ],
   "id": "8f032c61e06fff01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T12:38:57.346686Z",
     "start_time": "2025-05-07T12:38:57.340785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from setting.project_path import project_folder\n",
    "project_name='corona_discharge'\n",
    "main_folder=r\"D:\\my_project\"\n",
    "\n",
    "project_folder(project_review=project_name, main_folder=main_folder)"
   ],
   "id": "95782a760f85b5b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_folder': 'D:\\\\my_project',\n",
       " 'csv_path': 'D:\\\\my_project\\\\database\\\\corona_discharge_database.xlsx'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📥 Third Step: Download the Scopus BibTeX File\n",
    "\n",
    "In this step, you'll use the **Scopus database** to find and download relevant papers for your project. Scopus is a comprehensive and widely-used repository of peer-reviewed academic literature.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Using Scopus Advanced Search\n",
    "\n",
    "To retrieve high-quality and relevant papers, we recommend using **Scopus' Advanced Search** feature. This powerful tool lets you refine your search based on:\n",
    "\n",
    "* Keywords\n",
    "* Authors\n",
    "* Publication dates\n",
    "* Document types\n",
    "* And more...\n",
    "\n",
    "This ensures that your literature collection is both targeted and comprehensive.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Get Keyword Ideas with a Prompt\n",
    "\n",
    "To help you formulate effective search queries, you can use the following **prompt-based suggestion tool**:\n",
    "\n",
    "👉 [Keyword Search Prompt](https://gist.github.com/balandongiv/886437963d38252e61634ddc00b9d983)\n",
    "\n",
    "You may need to modify the prompt to better suit your research domain. Here are some example domains:\n",
    "\n",
    "* `\"corona discharge\"`\n",
    "* `\"fatigue driving EEG\"`\n",
    "* `\"wafer classification\"`\n",
    "\n",
    "Feel free to add, remove, or tweak keywords as needed to refine your search results.\n",
    "\n",
    "---\n",
    "\n",
    "## 💾 Save and Organize Your Results\n",
    "\n",
    "Once you've finalized your search:\n",
    "\n",
    "1. **Select all available attributes** when exporting results from Scopus.\n",
    "2. # Access Scopus\n",
    "![Scopus CSV Export](../image/scopus_csv_export.png)\n",
    "\n",
    "\n",
    "2. Choose the **BibTeX** format when saving the export file.\n",
    "3. Save the file inside the `database/scopus/` folder of your project.\n",
    "\n",
    "The resulting folder structure might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── scopus(1).bib\n",
    "│       ├── scopus(2).bib\n",
    "│       ├── scopus(3).bib\n",
    "```\n",
    "\n",
    "Make sure the BibTeX files are correctly named and stored to ensure smooth integration in later steps."
   ],
   "id": "72c1fb6183a242ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📊 Fourth Step: Combine Scopus BibTeX Files into Excel\n",
    "\n",
    "Once you've downloaded multiple `.bib` files from Scopus, the next step is to **combine and convert** them into a structured Excel file. This makes it easier to filter, sort, and review the metadata of all collected papers.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 What This Step Does\n",
    "\n",
    "* Loads all `.bib` files from your project's `database/scopus/` folder\n",
    "* Parses the relevant metadata (e.g., title, authors, year, source, DOI)\n",
    "* Combines the results into a single Excel spreadsheet\n",
    "* Saves the spreadsheet in the `database/` folder as `combined_filtered.xlsx`\n",
    "\n",
    "\n",
    "\n",
    "## 📁 Folder Structure Example\n",
    "\n",
    "After running the script, your folder might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   ├── scopus/\n",
    "│   │   ├── scopus(1).bib\n",
    "│   │   ├── scopus(2).bib\n",
    "│   │   ├── scopus(3).bib\n",
    "│   └── combined_filtered.xlsx\n",
    "```\n",
    "\n",
    "This Excel file will serve as your primary reference for filtering papers before downloading PDFs.\n"
   ],
   "id": "b5539ae56b1485ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T16:51:13.108451Z",
     "start_time": "2025-05-07T16:51:12.527576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from download_pdf.database_preparation import combine_scopus_bib_to_excel\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "project_review='corona_discharge'\n",
    "path_dic=project_folder(project_review=project_review)\n",
    "folder_path=path_dic['scopus_path']\n",
    "output_excel =  path_dic['database_path']\n",
    "combine_scopus_bib_to_excel(folder_path, output_excel)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 .bib files in D:\\my_project\\database\\scopus\n",
      "Initial number of rows: 7\n",
      "Number of rows after duplicate removal: 5. Total duplicates removed: 2\n",
      "These are the unique publisher_long values: ['Nature Research' 'Springer Science and Business Media B.V.'\n",
      " 'BioMed Central Ltd']\n",
      "['nature' 'springer' 'biomedcentral']\n",
      "Combined file has been saved to: D:\\my_project\\database\\corona_discharge_database.xlsx\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Sixth Step: Download PDFs**\n",
    "\n",
    "This step automates downloading PDFs for filtered academic papers using Selenium. The script uses metadata from `combined_filtered.xlsx` and saves each PDF with a filename based on its **BibTeX citation key**, ensuring consistent and human-readable identification.\n",
    "\n",
    "> 🛑 **Note:** This process launches a real browser window; some publishers block headless downloads.\n",
    "\n",
    "> 📝 **Important:** Only **Sci-Hub** is enabled by default. To activate fallback downloads from specific sources (e.g., IEEE, MDPI, ScienceDirect), you must **manually uncomment the corresponding function calls** in the script. This gives you full control over which fallback methods are used.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Loads filtered paper metadata from Excel (`combined_filtered.xlsx`).\n",
    "* Attempts to download PDFs from **Sci-Hub**.\n",
    "* Falls back to other publishers (IEEE, MDPI, ScienceDirect) **if you uncomment those lines**.\n",
    "* **Saves each PDF as `bibtex_key.pdf`** for easy referencing.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Code Snippet\n",
    "\n",
    "```python\n",
    "from download_pdf.download_pdf import (\n",
    "    run_pipeline,\n",
    "    process_scihub_downloads,\n",
    "    process_fallback_ieee,\n",
    "    # process_fallback_ieee_search,\n",
    "    # process_fallback_mdpi,\n",
    "    # process_fallback_sciencedirect,\n",
    ")\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Project setup\n",
    "project_review = 'wafer_defect'\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Load filtered Excel file\n",
    "file_path = path_dic['csv_path']\n",
    "output_folder = os.path.join(main_folder, 'pdf')\n",
    "\n",
    "# Load data and categorize by source\n",
    "categories, data_filtered = run_pipeline(file_path)\n",
    "\n",
    "# Step 1: Always try downloading from Sci-Hub first\n",
    "process_scihub_downloads(categories, output_folder, data_filtered)\n",
    "\n",
    "# Optional fallbacks — uncomment to enable each one:\n",
    "# Step 2: IEEE fallback\n",
    "# process_fallback_ieee(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 3: IEEE Search fallback\n",
    "# process_fallback_ieee_search(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 4: MDPI fallback\n",
    "# process_fallback_mdpi(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 5: ScienceDirect fallback (may not work due to security restrictions)\n",
    "# process_fallback_sciencedirect(categories, data_filtered, output_folder)\n",
    "\n",
    "# Optionally save the updated Excel\n",
    "# save_data(data_filtered, file_path)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Folder Structure Example\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       └── combined_filtered.xlsx     ← Filtered metadata with BibTeX keys\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                 ← Saved using bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "```"
   ],
   "id": "99f29be5d0a2412b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "from download_pdf.download_pdf import run_pipeline, process_scihub_downloads, process_fallback_ieee\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "file_path = path_dic['database_path']\n",
    "output_folder = os.path.join(main_folder, 'pdf')\n",
    "\n",
    "# Run the main pipeline to load and categorize the data\n",
    "categories, data_filtered = run_pipeline(file_path)\n",
    "\n",
    "# First step, we will always use Sci-Hub to attempt PDF downloads\n",
    "process_scihub_downloads(categories, output_folder, data_filtered)\n",
    "\n",
    "# Fallback options for entries not available via Sci-Hub:\n",
    "# Uncomment the following lines one by one if you want to try downloading from specific sources\n",
    "\n",
    "# Uncomment to attempt fallback download from IEEE\n",
    "# process_fallback_ieee(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download using IEEE Search\n",
    "# process_fallback_ieee_search(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download from MDPI\n",
    "# process_fallback_mdpi(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download from ScienceDirect\n",
    "# Note: ScienceDirect URLs can be extracted but PDFs may not be downloadable due to security restrictions\n",
    "# process_fallback_sciencedirect(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to save the updated data to Excel after processing\n",
    "# save_data(data_filtered, file_path)"
   ],
   "id": "6a37458687d840b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Seventh Step: Convert PDFs to XML using GROBID (OPTIONAL)**\n",
    "\n",
    "After downloading the PDFs, the next step is to convert them into structured **TEI XML** format using [**GROBID**](https://grobid.readthedocs.io). This step enables downstream tasks like metadata extraction, reference parsing, and full-text analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 What This Step Does\n",
    "\n",
    "* Processes all PDF files in your `pdf/` directory.\n",
    "* Uses GROBID's **batch processing API**.\n",
    "* Saves the resulting XML files into the `xml/` folder (one `.xml` per `.pdf`).\n",
    "* Leverages Docker for fast, isolated execution.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Setup Requirements\n",
    "\n",
    "> 🛠️ **GROBID requires WSL + Docker on Windows**\n",
    "\n",
    "* You must have **WSL** installed (tested on **WSL2 with Ubuntu 22.04**).\n",
    "* You must have **Docker** installed and **running** before launching GROBID.\n",
    "\n",
    "---\n",
    "\n",
    "## 🐳 How to Install & Run GROBID\n",
    "\n",
    "1. **Pull the Docker image from Docker Hub**\n",
    "   Check for the [latest version](https://hub.docker.com/r/grobid/grobid/tags), or use the stable one:\n",
    "\n",
    "   ```bash\n",
    "   docker pull grobid/grobid:0.8.1\n",
    "   ```\n",
    "\n",
    "2. **Start the GROBID container in Ubuntu (WSL)**\n",
    "\n",
    "   Open your Ubuntu terminal and run:\n",
    "\n",
    "   ```bash\n",
    "   docker run --rm --gpus all --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.1\n",
    "   ```\n",
    "\n",
    "   > ✅ This exposes GROBID's REST API on `http://localhost:8070/`\n",
    "\n",
    "3. **Test it in your browser**\n",
    "\n",
    "   Open a browser (e.g., Firefox or Chrome) and navigate to:\n",
    "\n",
    "   ```\n",
    "   http://localhost:8070/\n",
    "   ```\n",
    "\n",
    "   You should see the GROBID interface.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Batch Conversion Command\n",
    "\n",
    "Once the GROBID service is running, you can convert all PDFs in your `pdf/` folder to XML using:\n",
    "\n",
    "```bash\n",
    "# From your project root (in WSL/Ubuntu)\n",
    "cd path/to/your/project\n",
    "\n",
    "# Create output folder if not exists\n",
    "mkdir -p xml\n",
    "\n",
    "# Run batch processing using curl\n",
    "curl -v --form \"input=@pdf/\" localhost:8070/api/processFulltextDocument -o xml/\n",
    "```\n",
    "\n",
    "Or, use a Python wrapper or script to iterate over PDFs and call GROBID’s REST API for more control.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Folder Structure After Conversion\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf\n",
    "│   └── kim_2019.pdf\n",
    "├── xml/\n",
    "│   ├── smith_2020.xml\n",
    "│   └── kim_2019.xml\n",
    "```\n"
   ],
   "id": "c3026c463e76dec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Eighth Step (Optional): Convert XML to JSON**\n",
    "\n",
    "This step converts GROBID-generated TEI XML files into structured JSON format. While optional, it can be helpful for reviewing document content, integrating into other tools, or preparing data to feed into an LLM.\n",
    "\n",
    "> 📝 **Note:** This step is **optional** — the main pipeline (`run_llm`) reads directly from XML. Use this conversion if you want to inspect or process JSON files instead.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Reads all `*.xml` files from the `xml/` directory.\n",
    "* Converts each into a corresponding `*.json` file (preserving the **BibTeX key as filename** for consistency).\n",
    "* Stores all JSON outputs in `xml/json/`.\n",
    "\n",
    "In addition, it handles and organizes special cases:\n",
    "\n",
    "* 📁 **`xml/json/no_intro_conclusion/`**: XML files where GROBID could not detect an *introduction* or *conclusion* section.\n",
    "* 📁 **`xml/json/untitled_section/`**: XML files where GROBID could not detect any section titles at all — these require manual checking.\n",
    "* 📄 Other successfully processed files are stored directly in `xml/json/`.\n",
    "\n",
    "---\n",
    "\n",
    "## ▶️ Example Code\n",
    "\n",
    "```python\n",
    "from setting.project_path import project_folder\n",
    "from grobid_tei_xml.xml_json import run_pipeline\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "\n",
    "# Convert all XML files in the specified folder to JSON\n",
    "run_pipeline(path_dic['xml_path'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Example Folder Structure After Conversion\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf\n",
    "│   └── kim_2019.pdf\n",
    "├── xml/\n",
    "│   ├── smith_2020.xml\n",
    "│   ├── kim_2019.xml\n",
    "│   └── json/\n",
    "│       ├── smith_2020.json\n",
    "│       ├── kim_2019.json\n",
    "│       ├── no_intro_conclusion/\n",
    "│       │   └── failed_paper1.json\n",
    "│       └── untitled_section/\n",
    "│           └── failed_paper2.json\n",
    "```"
   ],
   "id": "24dba46998c443c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from setting.project_path import project_folder\n",
    "from grobid_tei_xml.xml_json import run_pipeline\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "run_pipeline(path_dic['xml_path'])"
   ],
   "id": "17f7f677505cff0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 🧾 **Tenth Step: Export Filtered Excel to BibTeX**\n",
    "\n",
    "After reviewing and filtering your `combined_filtered.xlsx` file, you can convert the refined list of papers back into a **BibTeX file**. This can be helpful for citation management or integration with tools like LaTeX or reference managers.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Loads the filtered Excel file containing your selected papers\n",
    "* Converts the data back into BibTeX format\n",
    "* Saves the result to a `.bib` file for easy reuse or citation\n",
    "\n",
    "\n",
    "## 📁 File Structure Example\n",
    "\n",
    "```\n",
    "bib_example/\n",
    "├── combined_filtered.xlsx      ← Filtered Excel file with selected papers\n",
    "└── filtered_output.bib         ← Newly generated BibTeX file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ Notes\n",
    "\n",
    "* Ensure the Excel file has standard bibliographic columns like: `title`, `author`, `year`, `journal`, `doi`, etc.\n",
    "* The function `generate_bibtex()` maps these fields into valid BibTeX entries.\n",
    "* You can open the `.bib` file in any text editor or reference manager to confirm the results."
   ],
   "id": "3c8648d56cd6763f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:34:10.604208Z",
     "start_time": "2025-05-07T13:34:10.560713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from post_code_saviour.excel_to_bib import generate_bibtex\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Define input and output paths\n",
    "input_excel = os.path.join(main_folder, 'database', 'combined_filtered.xlsx')\n",
    "output_bib = os.path.join(main_folder, 'database', 'filtered_output.bib')\n",
    "\n",
    "# Load the filtered Excel file\n",
    "df = pd.read_excel(input_excel)\n",
    "\n",
    "# Generate BibTeX file\n",
    "generate_bibtex(df, output_file=output_bib)\n"
   ],
   "id": "35d9935f47aebf25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BibTeX file generated: D:\\my_project\\database\\filtered_output.bib\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
