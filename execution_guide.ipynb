{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📦 **Step 1: Install Required Packages + Configure API Keys**\n",
    "\n",
    "Before you start using this notebook, ensure all required Python packages are installed **and** that your API keys are properly set up in a `.env` file. This prepares your environment for accessing services like OpenAI or Google Gemini.\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 How to Install Required Packages\n",
    "\n",
    "Open your terminal or command prompt, navigate to the project directory (where `requirements.txt` is located), and run:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "> ✅ **Tip:** Use a virtual environment (e.g., `venv`, `conda`) to keep your dependencies clean and isolated.\n",
    "\n",
    "If you're running this from a Jupyter notebook:\n",
    "\n",
    "```python\n",
    "!pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔐 Set Up API Keys in `.env` File\n",
    "\n",
    "To keep your API keys safe and reusable, store them in a `.env` file at the root of your project.\n",
    "\n",
    "### 📄 Example `.env` File\n",
    "\n",
    "```env\n",
    "GEMINI_API_KEY=your_gemini_api_key_here\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "```\n",
    "\n",
    "> 🔑 **Get your API keys from:**\n",
    ">\n",
    "> * **Gemini (Google AI Studio):**\n",
    "> * **OpenAI (ChatGPT/GPT-4):**\n",
    "\n",
    "This `.env` file is read by the Python `dotenv` library, so make sure it's included in your `requirements.txt`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Troubleshooting\n",
    "\n",
    "* If you get a \"permission denied\" error:\n",
    "\n",
    "  ```bash\n",
    "  pip install --user -r requirements.txt\n",
    "  ```\n",
    "\n",
    "* Ensure `.env` is not shared or committed to version control (e.g., use `.gitignore`):\n",
    "\n",
    "  ```\n",
    "  .env\n",
    "  ```\n"
   ],
   "id": "388df311542bae67"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🔧 Second Step: Create and Register Your Project Folder\n",
    "\n",
    "In this step, you'll register the **main project folder**, so the system knows where to store and retrieve all files related to your project.\n",
    "\n",
    "This setup automatically creates a structured folder system under your specified `main_folder`, and stores the configuration in a central JSON file. This ensures your projects remain organized, especially when working with multiple studies or datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Project Folder Structure\n",
    "\n",
    "On the first run, the following folder structure will be automatically created under your specified `main_folder`:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│   └── <project_name>_database.xlsx  ← auto-generated path (not file creation)\n",
    "├── pdf/\n",
    "└── xml/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Example\n",
    "\n",
    "Suppose your project name is `corona_discharge`, and you want to store all project files under:\n",
    "\n",
    "```\n",
    "G:\\My Drive\\research_related\\ear_eog\n",
    "```\n",
    "\n",
    "You can register this setup by running:\n",
    "\n",
    "```python\n",
    "project_folder(\n",
    "    project_review='corona_discharge',\n",
    "    main_folder=r'G:\\My Drive\\research_related\\ear_eog'\n",
    ")\n",
    "```\n",
    "\n",
    "✅ This will:\n",
    "\n",
    "* Save the project path in `setting/project_folders.json`\n",
    "* Create the full folder structure: `database/scopus`, `pdf`, and `xml`\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 Loading the Project Later\n",
    "\n",
    "Once registered, you can access the project folders in future sessions by providing just the `project_review` name:\n",
    "\n",
    "```python\n",
    "paths = project_folder(project_review='corona_discharge')\n",
    "\n",
    "print(paths['main_folder'])  # Main project folder\n",
    "print(paths['csv_path'])     # Path to the Excel database file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ What Happens in the Background?\n",
    "\n",
    "* A file named `project_folders.json` is stored in the `setting/` directory within your project.\n",
    "* It maps each `project_review` to its corresponding `main_folder`.\n",
    "* Folder structure is created automatically on the first run.\n",
    "* On subsequent runs, the system reads the JSON to locate your project — no need to re-enter paths.\n"
   ],
   "id": "8f032c61e06fff01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T12:38:57.346686Z",
     "start_time": "2025-05-07T12:38:57.340785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from setting.project_path import project_folder\n",
    "project_name='corona_discharge'\n",
    "main_folder=r\"D:\\my_project\"\n",
    "\n",
    "project_folder(project_review=project_name, main_folder=main_folder)"
   ],
   "id": "95782a760f85b5b1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_folder': 'D:\\\\my_project',\n",
       " 'csv_path': 'D:\\\\my_project\\\\database\\\\corona_discharge_database.xlsx'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📥 Third Step: Download the Scopus BibTeX File\n",
    "\n",
    "In this step, you'll use the **Scopus database** to find and download relevant papers for your project. Scopus is a comprehensive and widely-used repository of peer-reviewed academic literature.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Using Scopus Advanced Search\n",
    "\n",
    "To retrieve high-quality and relevant papers, we recommend using **Scopus' Advanced Search** feature. This powerful tool lets you refine your search based on:\n",
    "\n",
    "* Keywords\n",
    "* Authors\n",
    "* Publication dates\n",
    "* Document types\n",
    "* And more...\n",
    "\n",
    "This ensures that your literature collection is both targeted and comprehensive.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Get Keyword Ideas with a Prompt\n",
    "\n",
    "To help you formulate effective search queries, you can use the following **prompt-based suggestion tool**:\n",
    "\n",
    "👉 [Keyword Search Prompt](https://gist.github.com/balandongiv/886437963d38252e61634ddc00b9d983)\n",
    "\n",
    "You may need to modify the prompt to better suit your research domain. Here are some example domains:\n",
    "\n",
    "* `\"corona discharge\"`\n",
    "* `\"fatigue driving EEG\"`\n",
    "* `\"wafer classification\"`\n",
    "\n",
    "Feel free to add, remove, or tweak keywords as needed to refine your search results.\n",
    "\n",
    "---\n",
    "\n",
    "## 💾 Save and Organize Your Results\n",
    "\n",
    "Once you've finalized your search:\n",
    "\n",
    "1. **Select all available attributes** when exporting results from Scopus.\n",
    "2. # Access Scopus\n",
    "![Scopus CSV Export](image/scopus_csv_export.png)\n",
    "\n",
    "\n",
    "2. Choose the **BibTeX** format when saving the export file.\n",
    "3. Save the file inside the `database/scopus/` folder of your project.\n",
    "\n",
    "The resulting folder structure might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       ├── scopus(1).bib\n",
    "│       ├── scopus(2).bib\n",
    "│       ├── scopus(3).bib\n",
    "```\n",
    "\n",
    "Make sure the BibTeX files are correctly named and stored to ensure smooth integration in later steps."
   ],
   "id": "72c1fb6183a242ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 📊 Fourth Step: Combine Scopus BibTeX Files into Excel\n",
    "\n",
    "Once you've downloaded multiple `.bib` files from Scopus, the next step is to **combine and convert** them into a structured Excel file. This makes it easier to filter, sort, and review the metadata of all collected papers.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 What This Step Does\n",
    "\n",
    "* Loads all `.bib` files from your project's `database/scopus/` folder\n",
    "* Parses the relevant metadata (e.g., title, authors, year, source, DOI)\n",
    "* Combines the results into a single Excel spreadsheet\n",
    "* Saves the spreadsheet in the `database/` folder as `combined_filtered.xlsx`\n",
    "\n",
    "\n",
    "\n",
    "## 📁 Folder Structure Example\n",
    "\n",
    "After running the script, your folder might look like this:\n",
    "\n",
    "```\n",
    "main_folder/\n",
    "├── database/\n",
    "│   ├── scopus/\n",
    "│   │   ├── scopus(1).bib\n",
    "│   │   ├── scopus(2).bib\n",
    "│   │   ├── scopus(3).bib\n",
    "│   └── combined_filtered.xlsx\n",
    "```\n",
    "\n",
    "This Excel file will serve as your primary reference for filtering papers before downloading PDFs.\n"
   ],
   "id": "b5539ae56b1485ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:56:17.598973Z",
     "start_time": "2025-05-07T18:56:17.126130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from download_pdf.database_preparation import combine_scopus_bib_to_excel\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "project_review='corona_discharge'\n",
    "path_dic=project_folder(project_review=project_review)\n",
    "folder_path=path_dic['scopus_path']\n",
    "output_excel =  path_dic['database_path']\n",
    "combine_scopus_bib_to_excel(folder_path, output_excel)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 .bib files in D:\\my_project\\database\\scopus\n",
      "Initial number of rows: 7\n",
      "Number of rows after duplicate removal: 5. Total duplicates removed: 2\n",
      "These are the unique publisher_long values: ['Nature Research' 'Springer Science and Business Media B.V.'\n",
      " 'BioMed Central Ltd']\n",
      "['nature' 'springer' 'biomedcentral']\n",
      "Combined file has been saved to: D:\\my_project\\database\\corona_discharge_database.xlsx\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Fifth Step: Filter Relevant References Using LLM**\n",
    "\n",
    "After retrieving thousands of BibTeX references from Scopus in **Step 3**, many entries may not be relevant to your research focus. While you could manually filter them in the `combined_filtered.xlsx` file (produced in **Step 4**), doing so for thousands of entries is impractical.\n",
    "\n",
    "This step leverages a **Large Language Model (LLM)** to automatically classify abstracts based on relevance, using a prompt tailored to your research goal — for example, detecting whether a paper is about **wafer classification**.\n",
    "\n",
    "> ✅ Prompt used in this example:\n",
    "> `abstract_wafer_abstract_filter`\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Reads abstracts from the `combined_filtered.xlsx` file.\n",
    "* Applies an LLM with a custom filtering prompt.\n",
    "* Appends a new column to the Excel file (named after the `agent_name`).\n",
    "* Labels each entry as:\n",
    "\n",
    "  * `True` → Relevant\n",
    "  * `False` → Not Relevant\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Output Details\n",
    "\n",
    "* The column name added to the Excel file matches `agent_name`, as defined in:\n",
    "\n",
    "```python\n",
    "agentic_setting = {\n",
    "    \"agent_name\": \"abstract_wafer_abstract_filter\",\n",
    "    \"column_name\": \"abstract_wafer_abstract_filter\",\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "* The result (`True` or `False`) indicates whether the abstract was deemed relevant by the model.\n",
    "\n",
    "* The output Excel file:\n",
    "\n",
    "  * If `overwrite_csv = True`: the **original Excel file will be overwritten**.\n",
    "  * If `overwrite_csv = False`: a **new Excel file** will be created in the same folder, with a modified filename (e.g., `combined_filtered_updated.xlsx` or similar).\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ **Important Remark: Manual Review is Still Required**\n",
    "\n",
    "> 🛑 **Do NOT blindly trust LLM outputs.**\n",
    ">\n",
    "> While this step dramatically reduces the manual workload, **LLMs can make incorrect judgments**. Some relevant abstracts might be marked as irrelevant, or vice versa.\n",
    ">\n",
    "> Always **manually verify the final filtered results**, especially before publication or analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## ▶️ Example Code\n",
    "\n",
    "```python\n",
    "from research_filter.auto_llm import run_pipeline\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Choose your LLM\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# Agent configuration\n",
    "agentic_setting = {\n",
    "    \"agent_name\": \"abstract_wafer_abstract_filter\",\n",
    "    \"column_name\": \"abstract_wafer_abstract_filter\",\n",
    "    \"yaml_path\": r\"C:\\Users\\balan\\IdeaProjects\\academic_paper_maker\\research_filter\\agent\\agent_ml.yaml\",\n",
    "    \"model_name\": model_name\n",
    "}\n",
    "\n",
    "# Paths and folders\n",
    "csv_path = path_dic['database_path']\n",
    "methodology_json_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'json_output')\n",
    "multiple_runs_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'multiple_runs_folder')\n",
    "final_cross_check_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'final_cross_check_folder')\n",
    "\n",
    "# LLM runtime settings\n",
    "process_setup = {\n",
    "    'batch_process': False,\n",
    "    'manual_paste_llm': False,\n",
    "    'iterative_confirmation': False,\n",
    "    'overwrite_csv': False,\n",
    "    'cross_check_enabled': False,\n",
    "    'cross_check_runs': 3,\n",
    "    'cross_check_agent_name': 'agent_cross_check',\n",
    "    'cleanup_json': False\n",
    "}\n",
    "\n",
    "# Run the LLM-based filtering\n",
    "run_pipeline(\n",
    "    agentic_setting,\n",
    "    process_setup,\n",
    "    placeholders={},\n",
    "    csv_path=csv_path,\n",
    "    main_folder=main_folder,\n",
    "    methodology_json_folder=methodology_json_folder,\n",
    "    multiple_runs_folder=multiple_runs_folder,\n",
    "    final_cross_check_folder=final_cross_check_folder,\n",
    ")\n",
    "```"
   ],
   "id": "712bb934a2db9134"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from research_filter.auto_llm import run_pipeline\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Choose your LLM\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "# Agent configuration\n",
    "agentic_setting = {\n",
    "    \"agent_name\": \"abstract_wafer_abstract_filter\",\n",
    "    \"column_name\": \"abstract_wafer_abstract_filter\",\n",
    "    \"yaml_path\": r\"C:\\Users\\balan\\IdeaProjects\\academic_paper_maker\\research_filter\\agent\\agent_ml.yaml\",\n",
    "    \"model_name\": model_name\n",
    "}\n",
    "\n",
    "# Paths and folders\n",
    "csv_path = path_dic['database_path']\n",
    "methodology_json_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'json_output')\n",
    "multiple_runs_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'multiple_runs_folder')\n",
    "final_cross_check_folder = os.path.join(main_folder, agentic_setting['agent_name'], 'final_cross_check_folder')\n",
    "\n",
    "# LLM runtime settings\n",
    "process_setup = {\n",
    "    'batch_process': False,\n",
    "    'manual_paste_llm': False,\n",
    "    'iterative_confirmation': False,\n",
    "    'overwrite_csv': False,\n",
    "    'cross_check_enabled': False,\n",
    "    'cross_check_runs': 3,\n",
    "    'cross_check_agent_name': 'agent_cross_check',\n",
    "    'cleanup_json': False\n",
    "}\n",
    "\n",
    "# Run the LLM-based filtering\n",
    "run_pipeline(\n",
    "    agentic_setting,\n",
    "    process_setup,\n",
    "    placeholders={},\n",
    "    csv_path=csv_path,\n",
    "    main_folder=main_folder,\n",
    "    methodology_json_folder=methodology_json_folder,\n",
    "    multiple_runs_folder=multiple_runs_folder,\n",
    "    final_cross_check_folder=final_cross_check_folder,\n",
    ")\n"
   ],
   "id": "7ea8ae2257b2a4d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Sixth Step: Download PDFs**\n",
    "\n",
    "Now that your reference list has been filtered to include only **relevant papers** (based on the abstract analysis in **Step 5**), you're ready to automatically download their corresponding PDFs.\n",
    "\n",
    "This step uses the filtered Excel file (updated in Step 5) to retrieve and save PDFs for each BibTeX entry. The script is powered by Selenium and supports fallback strategies for sources like IEEE, MDPI, and ScienceDirect.\n",
    "\n",
    "> 🛑 **Note:** This step launches a full browser window during execution. Some publishers may block headless downloads — using a visible browser avoids this issue.\n",
    "\n",
    "> 📝 **Important:** By default, only **Sci-Hub** is enabled. To use fallback sources like IEEE, MDPI, or ScienceDirect, you must **manually uncomment the relevant function calls** in the script. This allows you to selectively control which sources to attempt.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Loads metadata from the filtered Excel file (`combined_filtered.xlsx` or the updated version from Step 5).\n",
    "* Attempts to download each paper from **Sci-Hub** first.\n",
    "* If Sci-Hub fails for a paper, you can optionally enable **fallback downloads** from:\n",
    "\n",
    "  * IEEE\n",
    "  * IEEE Search\n",
    "  * MDPI\n",
    "  * ScienceDirect (note: may fail due to access restrictions)\n",
    "* Saves each PDF as `{bibtex_key}.pdf` in the `pdf/` directory for easy tracking and consistent file naming.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Code Snippet\n",
    "\n",
    "```python\n",
    "from download_pdf.download_pdf import (\n",
    "    run_pipeline,\n",
    "    process_scihub_downloads,\n",
    "    process_fallback_ieee,\n",
    "    # process_fallback_ieee_search,\n",
    "    # process_fallback_mdpi,\n",
    "    # process_fallback_sciencedirect,\n",
    ")\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Project setup\n",
    "project_review = 'wafer_defect'\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Use the filtered Excel from Step 5\n",
    "file_path = path_dic['csv_path']\n",
    "output_folder = os.path.join(main_folder, 'pdf')\n",
    "\n",
    "# Load and categorize data\n",
    "categories, data_filtered = run_pipeline(file_path)\n",
    "\n",
    "# Step 1: Attempt to download from Sci-Hub\n",
    "process_scihub_downloads(categories, output_folder, data_filtered)\n",
    "\n",
    "# Optional fallback sources — uncomment as needed:\n",
    "# Step 2: Try IEEE fallback if Sci-Hub fails\n",
    "# process_fallback_ieee(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 3: Use IEEE Search-based fallback\n",
    "# process_fallback_ieee_search(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 4: Try MDPI fallback\n",
    "# process_fallback_mdpi(categories, data_filtered, output_folder)\n",
    "\n",
    "# Step 5: Try ScienceDirect fallback (limited due to restrictions)\n",
    "# process_fallback_sciencedirect(categories, data_filtered, output_folder)\n",
    "\n",
    "# Optional: Save updated Excel with download statuses\n",
    "# save_data(data_filtered, file_path)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Example Folder Structure\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── database/\n",
    "│   └── scopus/\n",
    "│       └── combined_filtered.xlsx     ← Filtered metadata with BibTeX keys (updated in Step 5)\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf                 ← Saved using bibtex_key\n",
    "│   ├── kim_2019.pdf\n",
    "│   └── ...\n",
    "```\n",
    "\n",
    "---"
   ],
   "id": "99f29be5d0a2412b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "from download_pdf.download_pdf import run_pipeline, process_scihub_downloads, process_fallback_ieee\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "file_path = path_dic['database_path']\n",
    "output_folder = os.path.join(main_folder, 'pdf')\n",
    "\n",
    "# Run the main pipeline to load and categorize the data\n",
    "categories, data_filtered = run_pipeline(file_path)\n",
    "\n",
    "# First step, we will always use Sci-Hub to attempt PDF downloads\n",
    "process_scihub_downloads(categories, output_folder, data_filtered)\n",
    "\n",
    "# Fallback options for entries not available via Sci-Hub:\n",
    "# Uncomment the following lines one by one if you want to try downloading from specific sources\n",
    "\n",
    "# Uncomment to attempt fallback download from IEEE\n",
    "# process_fallback_ieee(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download using IEEE Search\n",
    "# process_fallback_ieee_search(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download from MDPI\n",
    "# process_fallback_mdpi(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to attempt fallback download from ScienceDirect\n",
    "# Note: ScienceDirect URLs can be extracted but PDFs may not be downloadable due to security restrictions\n",
    "# process_fallback_sciencedirect(categories, data_filtered, output_folder)\n",
    "\n",
    "# Uncomment to save the updated data to Excel after processing\n",
    "# save_data(data_filtered, file_path)"
   ],
   "id": "6a37458687d840b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Seventh Step: Convert PDFs to XML using GROBID (OPTIONAL)**\n",
    "\n",
    "After downloading the PDFs, the next step is to convert them into structured **TEI XML** format using [**GROBID**](https://grobid.readthedocs.io). This step enables downstream tasks like metadata extraction, reference parsing, and full-text analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 What This Step Does\n",
    "\n",
    "* Processes all PDF files in your `pdf/` directory.\n",
    "* Uses GROBID's **batch processing API**.\n",
    "* Saves the resulting XML files into the `xml/` folder (one `.xml` per `.pdf`).\n",
    "* Leverages Docker for fast, isolated execution.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Setup Requirements\n",
    "\n",
    "> 🛠️ **GROBID requires WSL + Docker on Windows**\n",
    "\n",
    "* You must have **WSL** installed (tested on **WSL2 with Ubuntu 22.04**).\n",
    "* You must have **Docker** installed and **running** before launching GROBID.\n",
    "\n",
    "---\n",
    "\n",
    "## 🐳 How to Install & Run GROBID\n",
    "\n",
    "1. **Pull the Docker image from Docker Hub**\n",
    "   Check for the [latest version](https://hub.docker.com/r/grobid/grobid/tags), or use the stable one:\n",
    "\n",
    "   ```bash\n",
    "   docker pull grobid/grobid:0.8.1\n",
    "   ```\n",
    "\n",
    "2. **Start the GROBID container in Ubuntu (WSL)**\n",
    "\n",
    "   Open your Ubuntu terminal and run:\n",
    "\n",
    "   ```bash\n",
    "   docker run --rm --gpus all --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.1\n",
    "   ```\n",
    "\n",
    "   > ✅ This exposes GROBID's REST API on `http://localhost:8070/`\n",
    "\n",
    "3. **Test it in your browser**\n",
    "\n",
    "   Open a browser (e.g., Firefox or Chrome) and navigate to:\n",
    "\n",
    "   ```\n",
    "   http://localhost:8070/\n",
    "   ```\n",
    "\n",
    "   You should see the GROBID interface.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Batch Conversion Command\n",
    "\n",
    "Once the GROBID service is running, you can convert all PDFs in your `pdf/` folder to XML using:\n",
    "\n",
    "```bash\n",
    "# From your project root (in WSL/Ubuntu)\n",
    "cd path/to/your/project\n",
    "\n",
    "# Create output folder if not exists\n",
    "mkdir -p xml\n",
    "\n",
    "# Run batch processing using curl\n",
    "curl -v --form \"input=@pdf/\" localhost:8070/api/processFulltextDocument -o xml/\n",
    "```\n",
    "\n",
    "Or, use a Python wrapper or script to iterate over PDFs and call GROBID’s REST API for more control.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Folder Structure After Conversion\n",
    "\n",
    "```\n",
    "wafer_defect/\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf\n",
    "│   └── kim_2019.pdf\n",
    "├── xml/\n",
    "│   ├── smith_2020.xml\n",
    "│   └── kim_2019.xml\n",
    "```\n"
   ],
   "id": "c3026c463e76dec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Eighth Step (Optional): Convert XML to JSON**\n",
    "\n",
    "This step converts GROBID-generated TEI XML files into structured JSON format. While optional, it can be helpful for reviewing document content, integrating into other tools, or preparing data to feed into an LLM.\n",
    "\n",
    "> 📝 **Note:** This step is **optional** — the main pipeline (`run_llm`) reads directly from XML. Use this conversion if you want to inspect or process JSON files instead.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Reads all `*.xml` files from the `xml/` directory.\n",
    "* Converts each into a corresponding `*.json` file (preserving the **BibTeX key as filename** for consistency).\n",
    "* Stores all JSON outputs in `xml/json/`.\n",
    "\n",
    "In addition, it handles and organizes special cases:\n",
    "\n",
    "* 📁 **`xml/json/no_intro_conclusion/`**: XML files where GROBID could not detect an *introduction* or *conclusion* section.\n",
    "* 📁 **`xml/json/untitled_section/`**: XML files where GROBID could not detect any section titles at all — these require manual checking.\n",
    "* 📄 Other successfully processed files are stored directly in `xml/json/`.\n",
    "\n",
    "---\n",
    "\n",
    "## ▶️ Example Code\n",
    "\n",
    "```python\n",
    "from setting.project_path import project_folder\n",
    "from grobid_tei_xml.xml_json import run_pipeline\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "\n",
    "# Convert all XML files in the specified folder to JSON\n",
    "run_pipeline(path_dic['xml_path'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Example Folder Structure After Conversion\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── pdf/\n",
    "│   ├── smith_2020.pdf\n",
    "│   └── kim_2019.pdf\n",
    "├── xml/\n",
    "│   ├── smith_2020.xml\n",
    "│   ├── kim_2019.xml\n",
    "│   └── json/\n",
    "│       ├── smith_2020.json\n",
    "│       ├── kim_2019.json\n",
    "│       ├── no_intro_conclusion/\n",
    "│       │   └── failed_paper1.json\n",
    "│       └── untitled_section/\n",
    "│           └── failed_paper2.json\n",
    "```"
   ],
   "id": "24dba46998c443c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from setting.project_path import project_folder\n",
    "from grobid_tei_xml.xml_json import run_pipeline\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "run_pipeline(path_dic['xml_path'])"
   ],
   "id": "17f7f677505cff0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Ninth Step: Extract Methodology Details Using LLM**\n",
    "\n",
    "At this stage, your reference list is filtered and the corresponding PDFs (or abstracts) are available. Now, the focus shifts to **extracting key methodological insights** from each paper, such as:\n",
    "\n",
    "* 🧠 Classification algorithms\n",
    "* 🛠️ Feature engineering approaches\n",
    "* 📏 Evaluation metrics\n",
    "\n",
    "This is achieved using a specialized **LLM agent** with a targeted prompt for methodology extraction.\n",
    "\n",
    "> 📌 This step works with **full manuscripts (PDF)** when available, and **falls back to abstracts** if no PDF exists. This flexibility ensures comprehensive analysis even with incomplete data.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Loads your filtered Excel file from Step 5 or 6.\n",
    "* For each relevant paper:\n",
    "\n",
    "  * If the PDF is available: attempts to extract methodology from the full text.\n",
    "  * If the PDF is **not** available: falls back to using the **abstract**.\n",
    "* Extracts domain-specific insights using a tailored LLM prompt.\n",
    "* Appends the results to the same Excel (or a new copy depending on settings).\n",
    "* Saves structured JSON output for each document under a dedicated folder.\n",
    "\n",
    "---\n",
    "\n",
    "## 📄 Prompt Purpose\n",
    "\n",
    "This step uses a **domain-aware analytical agent** designed to:\n",
    "\n",
    "> “Extract methodological details (e.g., classification algorithms, feature engineering, evaluation metrics) from filtered papers relevant to a specific machine learning task.”\n",
    "\n",
    "The prompt is defined in a YAML config (`agent_ml.yaml`) and is tailored by the agent name you supply.\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Output Structure Example\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── pdf/\n",
    "│   └── smith_2020.pdf\n",
    "├── xml/\n",
    "│   └── smith_2020.xml\n",
    "├── methodology_gap_extractor_partial_discharge/\n",
    "│   ├── json_output/\n",
    "│   │   └── smith_2020.json\n",
    "│   ├── multiple_runs_folder/\n",
    "│   └── final_cross_check_folder/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Ensure `used_abstract = True` if some papers are missing full PDFs.\n",
    "* 🛑 **Do not blindly trust** LLM-extracted methodologies — they may misinterpret or hallucinate. Always manually **verify critical results** before relying on them for analysis or publication."
   ],
   "id": "9f09bf489150aabb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from research_filter.auto_llm import run_pipeline\n",
    "from setting.project_path import project_folder\n",
    "import os\n",
    "\n",
    "# Define the project\n",
    "project_review = 'corona_discharge'\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Select your LLM model\n",
    "model_name = 'gemini-2.0-flash-thinking-exp-01-21'\n",
    "\n",
    "# Agent and config setup\n",
    "agent_name = \"methodology_gap_extractor_partial_discharge\"\n",
    "agentic_setting = {\n",
    "    \"agent_name\": agent_name,\n",
    "    \"column_name\": agent_name,\n",
    "    \"yaml_path\": \"agent/agent_ml.yaml\",\n",
    "    \"model_name\": model_name\n",
    "}\n",
    "\n",
    "# Output directories\n",
    "methodology_json_folder = os.path.join(main_folder, agent_name, 'json_output')\n",
    "multiple_runs_folder = os.path.join(main_folder, agent_name, 'multiple_runs_folder')\n",
    "final_cross_check_folder = os.path.join(main_folder, agent_name, 'final_cross_check_folder')\n",
    "\n",
    "# Excel input\n",
    "csv_path = path_dic['csv_path']\n",
    "\n",
    "# Topic placeholders (adjust per project)\n",
    "placeholders = {\n",
    "    \"topic\": \"EEG-based fatigue classification\",\n",
    "    \"topic_context\": \"neurophysiological analysis\"\n",
    "}\n",
    "\n",
    "# LLM runtime configuration\n",
    "process_setup = {\n",
    "    'batch_process': False,\n",
    "    'manual_paste_llm': False,\n",
    "    'iterative_confirmation': False,\n",
    "    'overwrite_csv': False,\n",
    "    'cross_check_enabled': False,\n",
    "    'cross_check_runs': 3,\n",
    "    'cross_check_agent_name': 'agent_cross_check',\n",
    "    'cleanup_json': False,\n",
    "    'used_abstract': True  # Always True to enable fallback to abstract if PDF is missing\n",
    "}\n",
    "\n",
    "# Run the pipeline\n",
    "run_pipeline(\n",
    "    agentic_setting,\n",
    "    process_setup,\n",
    "    placeholders=placeholders,\n",
    "    csv_path=csv_path,\n",
    "    main_folder=main_folder,\n",
    "    methodology_json_folder=methodology_json_folder,\n",
    "    multiple_runs_folder=multiple_runs_folder,\n",
    "    final_cross_check_folder=final_cross_check_folder,\n",
    ")"
   ],
   "id": "d4cc866c39eb8c2c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Tenth Step: Draft Literature Review (Chapter 2) Using Combined JSON**\n",
    "\n",
    "Once you've extracted methodological insights in Step 9, the next logical move is to **start organizing and drafting your literature review** (typically Chapter 2 of a thesis or paper). This step outlines how to **combine extracted results** into a single, structured file — and how to use that file to produce high-quality summaries, tables, and narrative drafts using an LLM.\n",
    "\n",
    "> 📌 This is a branching step — not everyone will follow this exact path — but it's a powerful way to move from **structured data → written draft** efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## ✨ What This Step Does\n",
    "\n",
    "* Collects individual methodology JSON files.\n",
    "* Merges them into a single, unified JSON (`combined_output.json`).\n",
    "* Prepares this JSON as input for:\n",
    "\n",
    "  * Google AI Studio (e.g., Gemini Pro)\n",
    "  * GPT-based agents\n",
    "  * Jupyter notebooks or drafting pipelines\n",
    "* Enables:\n",
    "\n",
    "  * Narrative generation for Chapter 2\n",
    "  * Thematic clustering of methods\n",
    "  * Structured tables summarizing key findings\n",
    "\n",
    "---\n",
    "\n",
    "## 🧰 Code Example: Combine JSONs\n",
    "\n",
    "```python\n",
    "import os\n",
    "from setting.project_path import project_folder\n",
    "from helper.combine_json import combine_json_files\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "path_dict = project_folder(project_review=project_review)\n",
    "\n",
    "input_dir = os.path.join(\n",
    "    path_dict['main_folder'],\n",
    "    r\"methodology_gap_extractor_partial_discharge\\json_output\\gemini-2.0-flash-thinking-exp-01-21_updated\"\n",
    ")\n",
    "output_file = os.path.join(path_dict['main_folder'], 'combined_output.json')\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "\n",
    "combine_json_files(\n",
    "    input_directory=input_dir,\n",
    "    output_file=output_file\n",
    ")\n",
    "\n",
    "print(f\"Combined JSON saved to: {output_file}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Output Structure Example\n",
    "\n",
    "```\n",
    "corona_discharge/\n",
    "├── combined_output.json      ← ✅ Master file for summarization and drafting\n",
    "├── methodology_gap_extractor_partial_discharge/\n",
    "│   └── json_output/\n",
    "│       └── *.json            ← Individual extracted documents\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Using LLM to Generate Summary Tables\n",
    "\n",
    "With the `combined_output.json`, you can prompt an LLM to create structured tables summarizing key findings. For example:\n",
    "\n",
    "> “Using the combined JSON, generate a table with the following columns:\n",
    "> **Author**, **Machine Learning Algorithm**, **Feature Types**, **Dataset Used**, and **Performance Metrics**.”\n",
    "\n",
    "This gives you a **quick-glance overview** of the landscape, helpful both for understanding trends and citing clearly in your literature review.\n",
    "\n",
    "### ✅ Sample Columns for Summary Table:\n",
    "\n",
    "* **Author / Year**\n",
    "* **ML Algorithm(s) Used**\n",
    "* **Features**\n",
    "* **Dataset / Source**\n",
    "* **Performance (Accuracy, F1, etc.)**\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ Drafting Strategy Tips\n",
    "\n",
    "You can also use LLM prompts like:\n",
    "\n",
    "> “Based on the combined JSON, write a summary paragraph comparing the top 3 machine learning techniques used for partial discharge classification.”\n",
    "\n",
    "Or:\n",
    "\n",
    "> “Generate an introduction section discussing the evolution of feature engineering techniques in this domain.”\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Important Reminders\n",
    "\n",
    "* ✅ Make sure all JSON structures are consistent before combining.\n",
    "* 📉 Segment or cluster the JSON by subdomain if the file becomes too large.\n",
    "* 🔍 Review all generated tables and text — LLMs are **tools**, not final authorities.\n",
    "\n",
    "---"
   ],
   "id": "411c0567bd535174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from setting.project_path import project_folder\n",
    "from helper.combine_json import combine_json_files\n",
    "\n",
    "project_review = 'corona_discharge'\n",
    "path_dict = project_folder(project_review=project_review)\n",
    "\n",
    "input_dir = os.path.join(\n",
    "    path_dict['main_folder'],\n",
    "    r\"methodology_gap_extractor_partial_discharge\\json_output\\gemini-2.0-flash-thinking-exp-01-21_updated\"\n",
    ")\n",
    "output_file = os.path.join(path_dict['main_folder'], 'combined_output.json')\n",
    "\n",
    "if not os.path.exists(input_dir):\n",
    "    raise FileNotFoundError(f\"Input directory not found: {input_dir}\")\n",
    "\n",
    "combine_json_files(\n",
    "    input_directory=input_dir,\n",
    "    output_file=output_file\n",
    ")\n",
    "\n",
    "print(f\"Combined JSON saved to: {output_file}\")\n"
   ],
   "id": "527d5be14321c3c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 🧾 **Eleventh Step: Export Filtered Excel to BibTeX**\n",
    "\n",
    "After reviewing and filtering your `combined_filtered.xlsx` file, you can convert the refined list of papers back into a **BibTeX file**. This can be helpful for citation management or integration with tools like LaTeX or reference managers.\n",
    "\n",
    "---\n",
    "\n",
    "# ✨ What This Step Does\n",
    "\n",
    "* Loads the filtered Excel file containing your selected papers\n",
    "* Converts the data back into BibTeX format\n",
    "* Saves the result to a `.bib` file for easy reuse or citation\n",
    "\n",
    "\n",
    "# 📁 File Structure Example\n",
    "\n",
    "```\n",
    "bib_example/\n",
    "├── combined_filtered.xlsx      ← Filtered Excel file with selected papers\n",
    "└── filtered_output.bib         ← Newly generated BibTeX file\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Notes\n",
    "\n",
    "* Ensure the Excel file has standard bibliographic columns like: `title`, `author`, `year`, `journal`, `doi`, etc.\n",
    "* The function `generate_bibtex()` maps these fields into valid BibTeX entries.\n",
    "* You can open the `.bib` file in any text editor or reference manager to confirm the results."
   ],
   "id": "3c8648d56cd6763f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T13:34:10.604208Z",
     "start_time": "2025-05-07T13:34:10.560713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from post_code_saviour.excel_to_bib import generate_bibtex\n",
    "from setting.project_path import project_folder\n",
    "\n",
    "# Define your project\n",
    "project_review = 'corona_discharge'\n",
    "\n",
    "# Load project paths\n",
    "path_dic = project_folder(project_review=project_review)\n",
    "main_folder = path_dic['main_folder']\n",
    "\n",
    "# Define input and output paths\n",
    "input_excel = os.path.join(main_folder, 'database', 'combined_filtered.xlsx')\n",
    "output_bib = os.path.join(main_folder, 'database', 'filtered_output.bib')\n",
    "\n",
    "# Load the filtered Excel file\n",
    "df = pd.read_excel(input_excel)\n",
    "\n",
    "# Generate BibTeX file\n",
    "generate_bibtex(df, output_file=output_bib)\n"
   ],
   "id": "35d9935f47aebf25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BibTeX file generated: D:\\my_project\\database\\filtered_output.bib\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
